STEP 1:-INSTALL THE REQUIRED LIBRARIES

pip install PyPDF2
sentence-transformers langchain
openai pinecone-client tabulate

STEP 2:-EXTRACT TEXT FROM PDF

import PyPDF2


def extract_text_from_pdf(pdf_path):

    """Extracts text from a PDF file."""

    with open(pdf_path, 'rb') as file:

        reader = PyPDF2.PdfReader(file)

        text = ""

        for page in reader.pages:

            text += page.extract_text()

    return text

STEP 3:- SPLITTING THE TEXT INTO SMALLER CHUNKS FOR EASE

def chunk_text(text, chunk_size=300):

    """Splits the text into smaller chunks."""

    words = text.split()

    return [" ".join(words[i:i + chunk_size])
 for i in range(0, len(words), chunk_size)]

STEP 4:-GENERATING EMBEDDINGS

from sentence_transformers import SentenceTransformer


model = SentenceTransformer('all-MiniLM-L6-v2')

def generate_embeddings(chunks):
        """Generates vector embeddings for each chunk."""

    return [model.encode(chunk).tolist() for chunk in chunks]

STEP 5 :-STORING THE EMBEDDINGD IN A VECTOR DATABASE

import pinecone


# Initialize Pinecone

PINECONE_API_KEY = 'your_pinecone_api_key'
PINECONE_ENV = 'your_pinecone_environment'  # 
Example: 'us-west1-gcp'

pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)


INDEX_NAME = "pdf-rag-index"

if INDEX_NAME not in pinecone.list_indexes():

    pinecone.create_index(INDEX_NAME, dimension=384)

index = pinecone.Index(INDEX_NAME)


def 
 store_embeddings_in_pinecone(chunks, embeddings, metadata_list):

    """Stores embeddings and metadata in Pinecone."""
 
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):

        metadata = {"chunk_id": i, "text": chunk, **metadata_list[i]}

        index.upsert([(f"doc_{i}", embedding, metadata)])

STEP 6:-QUERY HANDLING

def query_pinecone(query, top_k=5):

    """Queries Pinecone for the most relevant chunks."""

    query_embedding = model.encode(query).tolist()

    results = index.query(query_embedding, top_k=top_k, include_metadata=True)

    return results

STEP 7:- RESPONSE GENERATION

import openai



openai.api_key = 'your_openai_api_key'


def generate_response_with_llm(query, results):

    """Generates a response using retrieved chunks and an LLM."""

    context = "\n".join([match['metadata']['text'] for match in results['matches']])
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"

    response = openai.Completion.create(

                                                                                  engine="text-davinci-003",

                                                                                  prompt=prompt,
  
                                                                                  max_tokens=300

                                                                                  )

    return response.choices[0].text.strip()

STEP 8:-COMPARISON QUERIES:-
 
from tabulate import tabulate


def handle_comparison_query(query, results):

    """Processes and generates a structured comparison."""

    comparisons = []

    for match in results['matches']:

        comparisons.append({
 
                                                     "Document": match['metadata']['source'],

                                                    "Content": match['metadata']['text']

                                                    })

    return tabulate(comparisons, headers="keys", tablefmt="grid")

STEP 9:-MAIN EXECUTION

if _name_ == "_main_":

    pdf_files = ["file1.pdf", "file2.pdf"]  // Replace with your PDF file paths
 //   
   metadata_list = [{"source": f"File {i+1}"}
  for i in range(len(pdf_files))]

    
    print("Extracting and processing PDFs...")

    for i, pdf_file in enumerate(pdf_files):

        text = extract_text_from_pdf(pdf_file)

        chunks = chunk_text(text)

        embeddings = generate_embeddings(chunks)

        store_embeddings_in_pinecone(chunks, embeddings, [metadata_list[i]] * len(chunks))
                         print("Data ingestion complete.")

    
    while True:
 
       query = input("\nEnter your question (or type 'exit' to quit): ")

        if query.lower() == "exit":
 
           print("Goodbye!")

            break

        
        print("Searching for relevant chunks...")

        results = query_pinecone(query)
 
       
        if "compare" in query.lower():

            response = handle_comparison_query(query, results)

        else:

            response = generate_response_with_llm(query, results)

        
        print("\nResponse:")

        print(response)